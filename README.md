ğŸ› ï¸ NASA Turbofan Engine Remaining Useful Life (RUL) Prediction

This project focuses on predicting the Remaining Useful Life (RUL) of aircraft turbofan engines using the NASA CMAPSS (Commercial Modular Aero-Propulsion System Simulation) dataset.
It implements a complete machine learning pipeline including exploratory data analysis (EDA), feature engineering, classical machine learning models, and hyperparameter tuning.

The project follows a clean, reproducible ML workflow and is structured for academic evaluation and real-world scalability.

ğŸ“Œ Project Objectives

Understand degradation patterns in turbofan engines using sensor data

Perform structured EDA and feature engineering

Train and evaluate classical machine learning models

Prevent data leakage using engine-level cross-validation

Optimize model performance via hyperparameter tuning

Ensure reproducibility and clarity through modular code design


ğŸ“‚ Repository Structure
turbofan-ml-project-scaffold/
â”‚
â”œâ”€â”€ .github/                 # CI configuration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Original NASA CMAPSS dataset
â”‚   â”œâ”€â”€ interim/             # Intermediate artifacts
â”‚   â””â”€â”€ processed/           # Preprocessed datasets (.parquet)
â”‚
â”œâ”€â”€ notebooks/               # Jupyter notebooks (EDA, preprocessing, models)
â”‚   â”œâ”€â”€ 01_eda_baseline.ipynb
â”‚   â”œâ”€â”€ 01_eda_preprocessing.ipynb
â”‚   â”œâ”€â”€ 02_classical_models.ipynb
â”‚   â””â”€â”€ 03_hyperparameter_tuning.ipynb
â”‚
â”œâ”€â”€ scripts/                 # Script-based model execution
â”‚   â”œâ”€â”€ run_baseline.py
â”‚   â”œâ”€â”€ run_random_forest.py
â”‚   â””â”€â”€ run_lstm_torch.py
â”‚
â”œâ”€â”€ src/                     # Core source code
â”‚   â”œâ”€â”€ data/                # Data loaders
â”‚   â”œâ”€â”€ features/            # Feature engineering
â”‚   â”œâ”€â”€ models/              # ML models
â”‚   â””â”€â”€ visualization/       # Plotting utilities
â”‚
â”œâ”€â”€ reports/                 # Saved metrics and trained models
â”œâ”€â”€ slides/                  # Presentation materials
â”œâ”€â”€ docs/                    # Project documentation
â”œâ”€â”€ tests/                   # Basic tests
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE


ğŸ“Š Dataset

This project uses the NASA CMAPSS Turbofan Engine Degradation Dataset, which contains multivariate time-series sensor data from simulated aircraft engines.

Dataset Subsets

FD001

FD002

FD003

FD004

Each subset represents different operating conditions and fault modes.

ğŸ“ Official source:
https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/

ğŸ› ï¸ Environment Setup
Prerequisites

Python 3.9+

VS Code / Jupyter Notebook

(Optional) Google Colab for hyperparameter tuning

Installation
git clone https://github.com/<your-username>/turbofan-ml-project-scaffold.git
cd turbofan-ml-project-scaffold


Create and activate a virtual environment (recommended):

python -m venv venv


Windows

venv\Scripts\activate


Linux / macOS

source venv/bin/activate


Install dependencies:

pip install -r requirements.txt

â–¶ï¸ How to Run the Project
ğŸ”¹ Step 1: Exploratory Data Analysis (Baseline)

ğŸ“„ notebooks/01_eda_baseline.ipynb

Loads raw CMAPSS data

Performs initial EDA

Computes Remaining Useful Life (RUL)

Trains a simple baseline model

ğŸ”¹ Step 2: EDA Preprocessing & Feature Engineering

ğŸ“„ notebooks/01_eda_preprocessing.ipynb

Builds rolling and delta features

Handles missing values

Scales features

Saves processed datasets to data/processed/

ğŸ”¹ Step 3: Classical Machine Learning Models

ğŸ“„ notebooks/02_classical_models.ipynb

Trains Decision Tree, Random Forest, and SVR models

Uses GroupKFold to prevent engine-level data leakage

Evaluates models using MAE, RMSE, and RÂ²

ğŸ”¹ Step 4: Hyperparameter Tuning (Google Colab)

ğŸ“„ notebooks/03_hyperparameter_tuning.ipynb

Performs RandomizedSearchCV

Optimizes model hyperparameters

Executed in Google Colab for faster computation

ğŸ” Recommended Execution Order
01_eda_baseline.ipynb
        â†“
01_eda_preprocessing.ipynb
        â†“
02_classical_models.ipynb
        â†“
03_hyperparameter_tuning.ipynb

â–¶ï¸ Script-Based Execution (Optional)

Run models directly using scripts:

python scripts/run_baseline.py
python scripts/run_random_forest.py
python scripts/run_lstm_torch.py


Outputs are saved in the reports/ directory.

ğŸ§ª Reproducibility & Best Practices

Same dataset used across all stages

GroupKFold prevents data leakage

Fixed random seeds where applicable

Modular design for maintainability

ğŸ“ˆ Results & Outputs

Evaluation metrics stored as JSON in reports/

Trained models saved for reuse

Visualizations available via notebooks

ğŸ§‘â€âš–ï¸ Notes for Evaluators

EDA and model development performed locally (VS Code)

Hyperparameter tuning executed in Google Colab for efficiency

Project follows a structured ML pipeline aligned with academic standards
